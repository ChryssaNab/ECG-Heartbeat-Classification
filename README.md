# Classifying Pathological Heartbeats from <br > ECG Signals

### [**Contents**](#)
1. [Project Description](#descr)
1. [Setup](#setup)
2. [Data Configuration](#dataset)
3. [Execution](#execution)
4. [Pre-training](#pre_training)
5. [Baseline Classifiers](#baseline)
6. [Fine-tuning](#fine_tuning)
7. [Hyperparameter Tuning](#parameter_tuning)
8. [Team](#team)

---

### [**Project Description**](#) <a name="descr"></a>

In this project, we train deep Convolutional Neural Networks (CNNs) to perform binary classification of ECG beats to normal and abnormal. We use transfer learning in order to build models that are fine-tuned on specific patientsâ€™ data, after pre-training a generic network on a set of different ECGs selected from the MIT-BIH arrhythmia database. We then compare the
performance of the fine-tuned networks against that of individual networks, which are trained only on the ECG data of a single patient, in order to evaluate the overall efficacy of transfer learning on the given problem.

---

### [**Setup**](#) <a name="setup"></a>

**1.** We assume that Python3 is already installed on the system. This code has been tested on Python version 3.10, though it should also be compatible with earlier versions.

**2.** Clone this repository:

``` shell
$ git clone https://github.com/ChryssaNab/Machine_Learning_RUG.git
$ cd Machine_Learning_RUG
```

 **3.** Create a new Python environment and activate it:

``` shell
$ python3 -m venv env
$ source env/bin/activate
```

**4.** Modify the *requirements.txt* file: 

> If your machine **does NOT support** CUDA, add the following line at the top of the *requirements.txt* file:
>> --extra-index-url https://download.pytorch.org/whl/cpu
>
> If your machine **does support** CUDA, add the following line instead, replacing **115** with the CUDA version your machine supports:
>> --extra-index-url https://download.pytorch.org/whl/cu115

**5.** Install necessary requirements:

``` shell
$ pip install wheel
$ pip install -r requirements.txt
```

---

### [**Data Configuration**](#) <a name="dataset"></a>

Download the dataset from https://www.kaggle.com/datasets/mondejar/mitbih-database and copy the contents to the parent folder of this directory under a directory named *dataset/mitbih_database*.

---

### [**Execution**](#) <a name="execution"></a>
The primary execution script resides in the *main.py* file within the *src/* directory. To view the potential arguments for executing this script, use the following command:

``` shell
$ python3 src/main.py -h
```

---

### [**Pre-training**](#) <a name="pre_training"></a>

To perform supervised pre-training on all patients using the default settings, run the following command:

``` shell
$ python3 src/main.py --state pre-training [--args]
```

Executing this command initiates the pre-training phase, using the hyperparameters specified in the *opts.py* script. Resultantly, a folder named *output/* will be generated within the parent directory, housing model state checkpoints for each epoch and three log files encompassing metrics like loss, accuracy, and other evaluations for training, validation, and test sets. The output folder location can be specified using the `--output_path` flag, while other parameters in *opts.py* can be adjusted accordingly.

---

### [**Baseline Classifiers**](#) <a name="baseline"></a>

``` shell
$ python3 src/main.py --state individuals
```

This command will start the experiments in which we train the CNN for each patient of our curated subset of patients individually using the best parameter set we found with grid search. This will create the folder '../output/individuals', which will contain one folder for each patient, each one containing the same files as the ones described in the pre-training section.The output folder can be specified with the flag --output_path.

---

### [**Fine-tuning**](#) <a name="fine_tuning"></a>

``` shell
$ python3 src/main.py --state fine_tuning
```

This command will start the experiments in which we fine tune the best performing CNN generated by the pre-training phase to each patient in our curated subset. This will create the folder './output/fine_tuning', which will contain one folder for each patient, each one containing the same files as the ones described in the pre-training section.The output folder can be specified with the flag --output_path.

---

### [**Hyperparameter Tuning**](#) <a name="parameter_tuning"></a>

It is possible to tune the parameters of the experiments from the command line. However, not all parameters defined in opts.py can be changed without unpredictable results. The list of tunable parameters is:

- data_path: Specify the directory under which the dataset lies. Needs to be specified if the dataset is not copied to the directory named above.
- output_path: The folder where the checkpoints and log files will be created.
- selected_patients_fine_tuning: The list of patients that will be used for the experiment. Only applies to the baseline individual phase and the fine-tuning phase.
- weight_decay: The value of the weight decay parameter of the optimizer.
- n_epochs: The maximum number of epochs for which the experiment will run.
- batch_size: The batch size used for training
- learning_rate: The initial learning rate
- weighted_sampling: Whether weighed sampling is enabled or not

Note that for the _individuals_ and _fine-tuning_ phases, setting the parameters _batch\_size_, _learning\_rate_ and _weighted\_sampling_ will have no effect, since they will be overriden by the optimal paramer set for each patient.

---

### [**Team**](#) <a name="team"></a>

- [Chryssa Nampouri](https://github.com/ChryssaNab)
- [Philip Andreadis](https://github.com/philip-andreadis)
- Christodoulos Hadjichristodoulou
- Marios Souroulla
